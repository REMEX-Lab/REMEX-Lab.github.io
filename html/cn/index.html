<!DOCTYPE html>
<html lang="zh-CN">

  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <meta name="description" content="REMEX主页">
    <meta name="author" content="WeiQM">
    <link rel="icon" href="../../images/logo/RMX_16.ico">
    <title>REMEX - Remote sensing + Medical imaging + X-features</title>
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" href="../../style/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <!-- Custom styles for this template -->
    <link href="../../style/jquery.bxslider.css" rel="stylesheet">
    <link href="../../style/style.css" rel="stylesheet">
  </head>

  <body>
    <!-- Navigation -->
    <nav class="navbar navbar-inverse navbar-fixed-top">
      <div class="container">
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar">
          <span class="sr-only">Toggle navigation</span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          <span class="icon-bar"></span>
          </button>
        </div>
        <div id="navbar" class="collapse navbar-collapse">  
          <ul class="nav navbar-nav">
            <li class="active"><a href="index.html"><i class="fa fa-home"></i> 主页</a></li>
            <li><a href="people.html">团队成员</a></li>
            <li><a href="research.html">研究项目</a></li>
            <li><a href="publications.html">发表著作</a></li>
            <li><a href="downloads.html">下载专区</a></li>
            <li><a href="contact.html">联系我们</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li><a href="../../index.html">English</a></li>
            <li class="active"><a href="index.html">中文</a></li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li><a href="index.html"><img src="../../images/logo/logo_w.png" alt="Logo" width="80px"/></a></li>
          </ul>
        </div>
      </div>
    </nav>
    <div class="container">
    <header>
      <!--
      <a href="index.html"><img src="images/logo.png"  width="256px"></a>
      -->
    </header>
    <!--
    <section class="main-slider">
      <ul class="bxslider">
        <li><div class="slider-item"><img src="images/logo/logo_c.png" title="Logo" /><h2><a href="" title="Loge">New published !</a></h2></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_m.png" title="Logo" /><h2><a href="" title="Loge">New published !</a></h2></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_y.png" title="Logo" /><h2><a href="" title="Loge">New published !</a></h2></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_k.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_r.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_g.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
        <li><div class="slider-item"><img src="images/logo/logo_b.png" title="Logo" /><h3><a href="" title="Loge">New published !</a></h3></div></li>
      </ul>
    </section>
    -->
    <section>
      <div class="row">
        <!-- Main Page -->
        <div class="col-md-8">
          <introduce class="content-block">
            <div class="block-body">
              <img src="../../images/logo.png" alt="Logo" width="512px">
              <p><br>
                <b>REMEX</b> (<b>Re</b>mote sensing and <b>Me</b>dical imaging with <b>X</b>-features)
                  实验室由姜志国教授负责，团队成员包括谢凤英教授、赵丹培副教授、张浩鹏副教授、郑钰山副教授，主要研究方向包括图像处理、模式识别、深度学习等人工智能技术及其在航天遥感、医学图像等领域的应用。
                  实验室依托“数字媒体”北京市重点实验室、北京市生物医学工程高精尖创新中心和“航天器设计优化与动态模拟技术”教育部重点实验室三个省部级科研平台，
                  在遥感图像处理和分析、空间目标图像处理、医学成像分析等方向承担多项国家自然科学基金、国家重点研发计划、国防预研、973、863、高分辨率对地观测重大专项等国家级科研项目，研究成果丰硕。
                  实验室隶属北航宇航学院，招收“模式识别与智能系统（控制科学与工程）”和“电子信息”两个学科的学术和专业学位硕博士研究生。
              </p>
              
              <div class="block-image">
                <img src="../../images/photo/Team1.jpg" alt="Team photo">
              </div>
              
              <hr/>
<!--              <h3 align="middle"><font color="FF0000">【New】</font>北航REMEX实验室2022年夏令营活动已结束，未能参加的同学可以单独联系意向导师</h3>-->
                <h3 align="middle">热忱欢迎全国高校优秀大学生到北航REMEX实验室继续深造！</h3>
<!--              <div class="camp-new">-->
<!--                  &#12288;&#12288;为进一步扩大宣传，增进师生交流，为2022年接收推免生选拔优秀生源并吸引优秀应届本科毕业生报考，特组织本次活动，热忱欢迎全国高校优秀大学生到北航REMEX实验室继续深造！-->
<!--              </div>-->
<!--                为进一步扩大宣传，增进师生交流，为2022年接收推免生选拔优秀生源并吸引优秀应届本科毕业生报考，特组织本次活动，热忱欢迎全国高校优秀大学生到北航REMEX实验室继续深造！-->
              <h4 align="left">学生要求</h4>
                <div class="camp-new">
                  1. “双一流”、“985”或“211”重点高校信息类及相关专业本科三年级在校生，有突出成果或特殊专业特长的其他高校本科三年级在校生。<br \>
                  2. 学习成绩优秀，预计能在就读学校获得推荐免试名额或拟报考北航宇航学院研究生。<br \>
                  3. 对图像处理、模式识别、计算机视觉、机器学习等人工智能相关科学研究有浓厚兴趣，有较强科研潜力。<br \>
                  4. 外语水平良好。<br \>
                  5. 身心健康。   <br \>
                    <h5>北航REMEX实验室2022年夏令营活动已结束，未能参加的同学可以单独联系意向导师。</h5>
                </div>
<!--              <h4 align="left">二、申请材料及提交方式</h4>-->
<!--                <div class="camp-new">-->
<!--                    1. 填写报名问卷：<a href="https://www.wjx.cn/vm/ex9mdOo.aspx">https://www.wjx.cn/vm/ex9mdOo.aspx</a>-->
<!--                    <div class="block-image">-->
<!--                        <img src="../../images/material/qc.jpg" alt="Team photo">-->
<!--                    </div>-->
<!--                    2. 结合本人实际情况，将个人简历、个人陈述、成绩单、相关证明材料（如学习或科研类主要的获奖证书、发表的研究论文首页、英语六级证书等）等合成为一份“姓名+本科学校.pdf”文件，以“夏令营报名+姓名+本科学校”为主题，发送邮件至buaazhp@126.com或其他意向导师邮箱。-->
<!--                </div>-->
<!--              <h4 align="left">三、活动内容及时间安排</h4>-->
<!--                <div class="camp-new">-->
<!--                    1. 报名截止日期：2022年7月20日（暂定），报名结果单独通知。<br \>-->
<!--                    2. 线上活动时间：2022年7月下旬（具体时间另行通知）。<br \>-->
<!--                    3. 活动内容：实验室研究方向介绍、师生交流等。<br \>-->
<!--                </div>-->
<!--              <h4 align="left">四、说明</h4>-->
<!--                <div class="camp-new">-->
<!--                    1. 本活动非官方活动，为REMEX实验室组织的师生交流活动，仅限意向导师为本实验室成员的学生参加，为加深师生相互了解提供平台。<br \>-->
<!--                    2. 学生须通过宇航学院研究生推免招生或统考招生方能获得北航录取资格。<br \>-->
<!--                    3. REMEX实验室主页：<a href="https://remex-lab.github.io/index.html">https://remex-lab.github.io/index.html</a><br \>-->
<!--                </div>-->
<!--                -->
<!--              <hr/>-->
              <h3 align="left">近期发表</h3><br />
              <div class="block-text">
              <table><tbody>

                <tr> <!-- An Paper -->
                  <p>
                      <b>Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification</b> <a href="" target="_blank"><i class="fa fa-external-link"></i></a>
                      <br>
                      <font size="3pt" face="Georgia"><i>
                          <a href="https://zhengyushan.github.io" target="_blank">Yushan Zheng*</a>, Jun Li, Jun Shi, Fengying Xie, Zhiguo Jiang
                      </i></font>
                      <br>
                      Medical Image Computing and Computer Assisted Intervention (MICCAI), 2022
                      <br>
                      <!-- <i class="fa fa-file-pdf-o"></i> <a href="https://zhengyushan.github.io" target="_blank">PDF</a> -->
                      <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('ZhengMICCAI2022Abs')">Abstract</a> &nbsp;
                      <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('ZhengMICCAI2022Bib')">BibTeX</a> &nbsp;
                      <i class="fa fa-github"></i> <a href="https://github.com/zhengyushan/kat" target="_blank">Code</a>
                  </p>
                  <p id="ZhengMICCAI2022Abs" class="abstract" style="display: none;">
                    Transformer has been widely used in histopathology whole slide image (WSI) classification for the purpose of tumor grading, prognosis analysis, etc. However, the design of token-wise self-attention and positional embedding strategy in the common Transformer limits the effectiveness and efficiency in the application to gigapixel histopathology images. In this paper, we propose a kernel attention Transformer (KAT) for histopathology WSI classification. The information transmission of the tokens is achieved by cross-attention between the tokens and a set of kernels related to a set of positional anchors on the WSI. Compared to the common Transformer structure, the proposed KAT can better describe the hierarchical context information of the local regions of the WSI and meanwhile maintains a lower computational complexity. The proposed method was evaluated on a gastric dataset with 2040 WSIs and an endometrial dataset with 2560 WSIs, and was compared with 5 state-of-the-art methods. The experimental results have demonstrated the proposed KAT is effective and efficient in the task of histopathology WSI classification and is superior to the state-of-the-art methods.
                  </p>
                  <pre xml:space="preserve" id="ZhengMICCAI2022Bib" class="bibtex" style="display: none;">
      @inproceedings{zheng2022kernel,
      author    = {Yushan Zheng, Jun Li, Jun Shi, Fengying Xie, Zhiguo Jiang},
      title     = {Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification},
      booktitle = {Medical Image Computing and Computer Assisted Intervention 
                  -- MICCAI 2022},
      year      = {2022}
      }
                  </pre>
                  <script language="javascript" type="text/javascript" xml:space="preserve">
                    hideblock('ZhengMICCAI2022Abs');
                    hideblock('ZhengMICCAI2022Bib');
                  </script>
                  </td>
              </tr> <!-- Paper End Here -->
      
              <tr> <!-- An Paper -->
                  <p>
                      <b>Lesion-Aware Contrastive Representation Learning For Histopathology Whole Slide Images Analysis</b> <a href="" target="_blank"><i class="fa fa-external-link"></i></a>
                      <br>
                      <font size="3pt" face="Georgia"><i>
                          Jun Li, <a href="https://zhengyushan.github.io" target="_blank">Yushan Zheng*</a>, Kun Wu, Jun Shi*, Fengying Xie, Zhiguo Jiang
                      </i></font>
                      <br>
                        Medical Image Computing and Computer Assisted Intervention (MICCAI), 2022
                      <br>
                      <!-- <i class="fa fa-file-pdf-o"></i> <a href="https://zhengyushan.github.io" target="_blank">PDF</a> -->
                      <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('LiMICCAI2022Abs')">Abstract</a> &nbsp;
                      <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('LiMICCAI2022Bib')">BibTeX</a> &nbsp;
                      <!-- <i class="fa fa-github"></i> <a href="" target="_blank">Code</a> -->
                  </p>
                  <p id="LiMICCAI2022Abs" class="abstract" style="display: none;">
                      Local representation learning has been a key challenge to promote the performance of the histopathological whole slide images analysis. The previous representation learning methods almost follow the supervised learning paradigm. However, manual annotation for large-scale WSIs is time-consuming and labor-intensive. Hence, the self-supervised contrastive learning has recently attracted intensive attention. Contrastive learning on the design of informative of the positive and negative pairs, but common contrastive learning methods treat each sample as a single class, which leads to the class collision problems, especially in the domain of histopathology image analysis. In this paper, we proposed a novel contrastive representation learning framework called Lesion-Aware Contrastive Learning (LACL) for histopathology whole slide image analysis. We built a module called lesion queue based on the memory bank structure to store the representations of different classes of WSIs, which allowed the model to selectively sample the negative pairs during the training. Moreover, We designed a queue refinement strategy to purify the representations stored in the lesion queue. The experimental results demonstrate that LACL achieves the best performance in histopathology image representation learning on different datasets, and outperforms state-of-the-art methods under different WSI classification benchmarks.
                  </p>
                  <pre xml:space="preserve" id="LiMICCAI2022Bib" class="bibtex" style="display: none;">
      @inproceedings{li2022lesion,
      author    = {Jun Li, Yushan Zheng*, Kun Wu, Jun Shi, Fengying Xie, Zhiguo Jiang},
      title     = {Lesion-Aware Contrastive Representation Learning For Histopathology Whole Slide Images Analysis},
      booktitle = {Medical Image Computing and Computer Assisted Intervention 
                    -- MICCAI 2022},
      year      = {2022}
      }
                  </pre>
                  <script language="javascript" type="text/javascript" xml:space="preserve">
                    hideblock('LiMICCAI2022Abs');
                    hideblock('LiMICCAI2022Bib');
                  </script>
                  </td>
              </tr> <!-- Paper End Here -->
      
              <tr> <!-- An Paper -->
                  <p>
                      <b>Multi-Frame Super-Resolution With Raw Images Via Modified Deformable Convolution</b> <a href="https://ieeexplore.ieee.org/abstract/document/9747407" target="_blank"><i class="fa fa-external-link"></i></a>
                      <br>
                      <font size="3pt" face="Georgia"><i>
                          Gongzhe Li, Linwei Qiu, <a href="https://haopzhang.github.io" target="_blank">Haopeng Zhang</a>, Fengying Xie, Zhiguo Jiang
                      </i></font>
                      <br>
                      IEEE ICASSP, 2022
                      <br>
                      <!-- <i class="fa fa-file-pdf-o"></i> <a href="" target="_blank">PDF</a> -->
                      <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('ZhangICASSP2022Abs')">Abstract</a> &nbsp;
                      <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('ZhangICASSP2022Bib')">BibTeX</a> &nbsp;
                      <!-- <i class="fa fa-github"></i> <a href="" target="_blank">Code</a> -->
                  </p>
                  <p id="ZhangICASSP2022Abs" class="abstract" style="display: none;">
                    In this paper we propose a novel model towards multi-frame super-resolution, which leverages multiple RAW images and yields a super-resolved RGB image. To facilitate the pixel misalignment in burst photography, we apply a refined Pyramid Cascading and Deformable Convolution (PCD) feature alignment module. A new 3D deformable convolution fusion module is proposed subsequently to merge the information from all frames adaptively. In addition, we employ an encoder-decoder network to restore color and details in sRGB space after super-resolving images in linear space. Extensive experiments demonstrate the superiority of our architecture and the strength of multi-frame super-resolution with RAW images.
                  </p>
                  <pre xml:space="preserve" id="ZhangICASSP2022Bib" class="bibtex" style="display: none;">
      @inproceedings{li2022multi,
        author={Li, Gongzhe and Qiu, Linwei and Zhang, Haopeng and Xie, Fengying and Jiang, Zhiguo},
        booktitle={ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
        title={Multi-Frame Super-Resolution With Raw Images Via Modified Deformable Convolution}, 
        year={2022},
        pages={2155-2159},
        doi={10.1109/ICASSP43922.2022.9747407}
      }
                  </pre>
                  <script language="javascript" type="text/javascript" xml:space="preserve">
                    hideblock('ZhangICASSP2022Abs');
                    hideblock('ZhangICASSP2022Bib');
                  </script>
                  </td>
              </tr> <!-- Paper End Here -->
      
              <tr> <!-- An Paper -->
                  <p>
                      <b>A Two-Stage Shake-Shake Network for Long-tailed Recognition of SAR Aerial View Objects</b> <a href="https://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Li_A_Two-Stage_Shake-Shake_Network_for_Long-Tailed_Recognition_of_SAR_Aerial_CVPRW_2022_paper.pdf" target="_blank"><i class="fa fa-external-link"></i></a>
                      <br>
                      <font size="3pt" face="Georgia"><i>
                        Gongzhe Li, Linpeng Pan, Linwei Qiu, Zhiwen Tan, Fengying Xie, <a href="https://haopzhang.github.io" target="_blank">Haopeng Zhang</a>*
                      </i></font>
                      <br>
                        18th IEEE Workshop on Perception Beyond the Visible Spectrum (PBVS Workshop 2022) in conjunction with CVPR 2022
                      <br>
                      <i class="fa fa-file-pdf-o"></i> <a href="https://openaccess.thecvf.com/content/CVPR2022W/PBVS/papers/Li_A_Two-Stage_Shake-Shake_Network_for_Long-Tailed_Recognition_of_SAR_Aerial_CVPRW_2022_paper.pdf" target="_blank">PDF</a>
                      <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('ZhangPBVS2022Abs')">Abstract</a> &nbsp;
                      <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('ZhangPBVS2022Bib')">BibTeX</a> &nbsp;
                      <i class="fa fa-github"></i> <a href="https://github.com/LinpengPan/PBVS2022-Multi-modal-AVOC-Challenge-Track1" target="_blank">Code</a>
                  </p>
                  <p id="ZhangPBVS2022Abs" class="abstract" style="display: none;">
                    Synthetic Aperture Radar (SAR) has received more attention due to its complementary superiority on capturing
                      significant information in the remote sensing area. However, for an Aerial View Object Classification (AVOC) task,
                      SAR images still suffer from the long-tailed distribution of
                      the aerial view objects. This disparity limit the performance
                      of classification methods, especially for the data-sensitive
                      deep learning models. In this paper, we propose a twostage shake-shake network to tackle the long-tailed learning problem. Specifically, it decouples the learning procedure into the representation learning stage and the classification learning stage. Moreover, we apply the test time
                      augmentation (TTA) and the classification with alternating
                      normalization (CAN) to improve the accuracy. In the PBVS
                      1 2022 Multi-modal Aerial View Object Classification Challenge Track 1, our method achieves 21.82% and 27.97% accuracy in the development phase and testing phase respectively, which wins the top-tier among all the participants
                  </p>
                  <pre xml:space="preserve" id="ZhangPBVS2022Bib" class="bibtex" style="display: none;">
      @inproceedings{
      author={Gongzhe Li, Linpeng Pan, Linwei Qiu, Zhiwen Tan, Fengying Xie, Haopeng Zhang},
      booktitle={18th IEEE Workshop on Perception Beyond the Visible Spectrum (PBVS Workshop 2022) in conjunction with CVPR 2022}, 
      title={A Two-Stage Shake-Shake Network for Long-tailed Recognition of SAR Aerial View Objects}, 
      year={2022},
      pages={245-256},
      }
                  </pre>
                  <script language="javascript" type="text/javascript" xml:space="preserve">
                    hideblock('ZhangPBVS2022Abs');
                    hideblock('ZhangPBVS2022Bib');
                  </script>
                  </td>
              </tr> <!-- Paper End Here -->
      
              <tr> <!-- An Paper -->
                  <p>
                      <b>Few-Shot Multi-Class Ship Detection in Remote Sensing Images Using Attention Feature Map and Multi-Relation Detector</b> <a href="https://www.mdpi.com/2072-4292/14/12/2790" target="_blank"><i class="fa fa-external-link"></i></a>
                      <br>
                      <font size="3pt" face="Georgia"><i>
                          <a href="https://haopzhang.github.io" target="_blank">Haopeng Zhang</a>*, Xingyu Zhang, Gang Meng, Chen Guo, Zhiguo Jiang
                      </i></font>
                      <br>
                        Remote Sensing, 2022
                      <br>
                      <i class="fa fa-file-pdf-o"></i> <a href="../../source/pdf/article_zhang_remotesensing_2022.pdf" target="_blank">PDF</a>
                      <i class="fa fa-bookmark-o"></i> <a href="javascript:toggleblock('ZhangRS2022Abs')">Abstract</a> &nbsp;
                      <i class="fa fa-quote-left"></i> <a href="javascript:toggleblock('ZhangRS2022Bib')">BibTeX</a> &nbsp;
                      <!-- <i class="fa fa-github"></i> <a href="" target="_blank">Code</a> -->
                  </p>
                  <p id="ZhangRS2022Abs" class="abstract" style="display: none;">
                      Monitoring and identification of ships in remote sensing images is of great significance for port management, marine traffic, marine security, etc. However, due to small size and complex background, ship detection in remote sensing images is still a challenging task. Currently, deep-learning-based detection models need a lot of data and manual annotation, while training data containing ships in remote sensing images may be in limited quantities. To solve this problem, in this paper, we propose a few-shot multi-class ship detection algorithm with attention feature map and multi-relation detector (AFMR) for remote sensing images. We use the basic framework of You Only Look Once (YOLO), and use the attention feature map module to enhance the features of the target. In addition, the multi-relation head module is also used to optimize the detection head of YOLO. Extensive experiments on publicly available HRSC2016 dataset and self-constructed REMEX-FSSD dataset validate that our method achieves a good detection performance.
                  </p>
                  <pre xml:space="preserve" id="ZhangRS2022Bib" class="bibtex" style="display: none;">
      @Article{zhang2022few,
      AUTHOR = {Zhang, Haopeng and Zhang, Xingyu and Meng, Gang and Guo, Chen and Jiang, Zhiguo},
      TITLE = {Few-Shot Multi-Class Ship Detection in Remote Sensing Images Using Attention Feature Map and Multi-Relation Detector},
      JOURNAL = {Remote Sensing},
      VOLUME = {14},
      YEAR = {2022},
      NUMBER = {12},
      ARTICLE-NUMBER = {2790},
      URL = {https://www.mdpi.com/2072-4292/14/12/2790},
      ISSN = {2072-4292},
      DOI = {10.3390/rs14122790}
      }
                  </pre>
                  <script language="javascript" type="text/javascript" xml:space="preserve">
                    hideblock('ZhangRS2022Abs');
                    hideblock('ZhangRS2022Bib');
                  </script>
                  </td>
              </tr> <!-- Paper End Here -->

                </tbody></table>
              </div>
              <div class="get-more" align="right"><a href="publications.html"> 更多 </a></div>
            </div>
          </introduce>
        </div>
        <!-- Slid Page -->
        <div class="col-md-4 sidebar-gutter">
          <aside>
          <!-- sidebar-widget -->
          <div class="sidebar-widget">
            <div class="widget-container widget-main">
              <img src="../../images/photo/JiangZG.jpg" alt="JiangZG's photo">
              <h4>姜志国</h4>
              <div class="author-title">教授</div>
              <p>
                <b>地址:</b> 北京市昌平区沙河高教园南三街9号<br>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<br> &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp邮编102206<br>
                <b>邮箱:</b> <a href="mailto:jiangzg@buaa.edu.cn">jiangzg@buaa.edu.cn</a><br>
<!--
                <b>电话:</b> TBA<br>
                <b>传真:</b> TBA<br>
-->
                <b>办公:</b> 主楼D721<br>
              </p>
            </div>
          </div>
          <!-- sidebar-widget -->
          <div class="sidebar-widget">
            <h3 class="sidebar-title">团队成员</h3>
            <div class="widget-container">
              <article class="widget-block">
                <div class="block-image"> <img src="../../images/photo/ZhangHP.jpg" alt="ZhangHP's photo"> </div>
                <div class="block-body">
                  <h2><a href="http://teacher.buaa.edu.cn/zhanghaopeng/" target="_blank">张浩鹏 <i class="fa fa-external-link"></i></a></h2>
                  <div class="icon-meta">
                    <span><i class="fa fa-graduation-cap"></i>副教授</span> <span><i class="fa fa-clock-o"></i> </span>
                    <br><span><i class="fa fa-envelope-o"></i> <a href="mailto:zhanghaopeng@buaa.edu.cn">zhanghaopeng@buaa.edu.cn</a></span>
                  </div>
                </div>
              </article>
              <article class="widget-block">
                <div class="block-image"> <img src="../../images/photo/XieFY.jpg" alt="ZhangYS's photo"> </div>
                <div class="block-body">
                  <h2><a href="http://www.sa.buaa.edu.cn/info/1014/4773.htm" target="_blank">谢凤英 <i class="fa fa-external-link"></i></a></h2>
                  <div class="icon-meta">
                    <span><i class="fa fa-graduation-cap"></i>教授</span> <span><i class="fa fa-clock-o"></i> </span>
                    <br><span><i class="fa fa-envelope-o"></i> <a href="mailto:xfy_73@buaa.edu.cn">xfy_73@buaa.edu.cn</a></span>
                  </div>
                </div>
              </article>
              <article class="widget-block">
                <div class="block-image"> <img src="../../images/photo/ZhaoDP.jpg" alt="ZhangYS's photo"> </div>
                <div class="block-body">
                  <h2><a href="http://www.sa.buaa.edu.cn/info/1014/4780.htm" target="_blank">赵丹培 <i class="fa fa-external-link"></i></a></h2>
                  <div class="icon-meta">
                    <span><i class="fa fa-graduation-cap"></i>副教授</span> <span><i class="fa fa-clock-o"></i> </span>
                    <br><span><i class="fa fa-envelope-o"></i> <a href="mailto:zhaodanpei@buaa.edu.cn">zhaodanpei@buaa.edu.cn</a></span>
                  </div>
                </div>
              </article>
              <article class="widget-block">
                <div class="block-image"> <img src="../../images/photo/ZhengYS.jpg" alt="ZhangYS's photo"> </div>
                <div class="block-body">
                  <h2><a href="https://zhengyushan.github.io" target="_blank">郑钰山 <i class="fa fa-external-link"></i></a></h2>
                  <div class="icon-meta">
                    <span><i class="fa fa-graduation-cap"></i>副教授</span> <span><i class="fa fa-clock-o"></i> </span>
                    <br><span><i class="fa fa-envelope-o"></i> <a href="mailto:yszheng@buaa.edu.cn">yszheng@buaa.edu.cn</a></span>
                  </div>
                </div>
              </article>
            </div>
          </div>
          <!-- sidebar-widget -->
          <div class="sidebar-widget">
            <h3 class="sidebar-title">相关链接</h3>
            <div class="widget-container">
              <ul style="list-style: none; padding-left: 10px;">
                <!-- <li><i class="fa fa-external-link"></i> <a href="https://remex-lab.github.io/" target="_blank">REMEX </a></li> -->
                <li><i class="fa fa-external-link"></i> <a href="http://teacher.buaa.edu.cn/zhanghaopeng/" target="_blank">张浩鹏个人主页</a></li>
                <li><i class="fa fa-external-link"></i> <a href="http://www.sa.buaa.edu.cn/info/1014/4773.htm" target="_blank">谢凤英个人主页</a></li>
                <li><i class="fa fa-external-link"></i> <a href="http://www.sa.buaa.edu.cn/info/1014/4780.htm" target="_blank">赵丹培个人主页</a></li>
                <li><i class="fa fa-external-link"></i> <a href="https://zhengyushan.github.io" target="_blank">郑钰山个人主页</a></li> 
              </ul>
            </div>
          </div>
          </aside>
        </div>
      </div>
    </section>
    </div><!-- /.container -->

    <footer class="footer">
      <div class="footer-bottom">  
        <i class="fa fa-copyright"></i> Copyright 2018. All rights reserved.<br>
        <!-- <i class="fa fa-anchor"></i> <a href="index_x.html"><b>X！</b><i class="fa fa-sign-in"></i></a> -->
      </div>
      
    </footer>


    <!-- Bootstrap core JavaScript
      ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../../scripts/jquery.min.js"></script>
    <script src="../../scripts/bootstrap.min.js"></script>
    <script src="../../scripts/jquery.bxslider.js"></script>
    <script src="../../scripts/mooz.scripts.min.js"></script>
    <script src="../../scripts/togglehide.js"></script>
  </body>
</html>
